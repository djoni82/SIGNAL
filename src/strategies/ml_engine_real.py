# src/strategies/ml_engine_real.py
"""
Ð ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ML Ð´Ð²Ð¸Ð¶Ð¾Ðº Ñ Ð°Ð½ÑÐ°Ð¼Ð±Ð»ÐµÐ¼ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð±ÑƒÑÑ‚Ð¸Ð½Ð³Ð°.
Ð—Ð°Ð¼ÐµÐ½ÑÐµÑ‚ ÑÐ²Ñ€Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ MLEngine Ð½Ð° XGBoost + LightGBM + CatBoost.
"""
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
import numpy as np
import joblib
import os
from typing import Dict, List, Optional
import logging

logger = logging.getLogger(__name__)

class RealMLEngine:
    """
    ÐÐ½ÑÐ°Ð¼Ð±Ð»ÑŒ Ð¸Ð· 3 Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð±ÑƒÑÑ‚Ð¸Ð½Ð³Ð°.
    Ð’Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð¾Ðµ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ.
    """
    def __init__(self, model_path="models/"):
        self.models = {
            'xgb': None,
            'lgbm': None,
            'catboost': None
        }
        self.feature_columns = []
        # ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²ÐµÑÐ° (Ð¼Ð¾Ð¶Ð½Ð¾ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸)
        self.model_weights = {
            'xgb': 0.4,
            'lgbm': 0.3,
            'catboost': 0.3
        }
        self.model_path = model_path
        self._ensure_model_dir()
        self._load_models()

    def _ensure_model_dir(self):
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÐµÑÐ»Ð¸ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚"""
        os.makedirs(self.model_path, exist_ok=True)

    def _load_models(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð¾Ð²"""
        try:
            xgb_path = f"{self.model_path}xgb_model.json"
            lgbm_path = f"{self.model_path}lgbm_model.txt"
            cat_path = f"{self.model_path}catboost_model.cbm"
            feat_path = f"{self.model_path}features.pkl"
            
            if os.path.exists(xgb_path):
                self.models['xgb'] = xgb.XGBClassifier()
                self.models['xgb'].load_model(xgb_path)
                logger.info("âœ… XGBoost model loaded")
            
            if os.path.exists(lgbm_path):
                self.models['lgbm'] = lgb.Booster(model_file=lgbm_path)
                logger.info("âœ… LightGBM model loaded")
            
            if os.path.exists(cat_path):
                self.models['catboost'] = CatBoostClassifier()
                self.models['catboost'].load_model(cat_path)
                logger.info("âœ… CatBoost model loaded")
            
            if os.path.exists(feat_path):
                with open(feat_path, "rb") as f:
                    self.feature_columns = joblib.load(f)
                logger.info(f"âœ… Feature schema loaded: {len(self.feature_columns)} features")
                
        except Exception as e:
            logger.warning(f"âš ï¸  Could not load ML models: {e}. Training needed.")

    def train_models(self, X_train, y_train, X_val, y_val):
        """
        ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… 3 Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ….
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¸Ð· data_pipeline.py
        """
        logger.info("ðŸŽ“ Starting model training...")
        
        # 1. XGBoost
        logger.info("Training XGBoost...")
        self.models['xgb'] = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric='logloss',
            random_state=42,
            use_label_encoder=False
        )
        self.models['xgb'].fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        self.models['xgb'].save_model(f"{self.model_path}xgb_model.json")

        # 2. LightGBM
        logger.info("Training LightGBM...")
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.01,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42
        }
        
        self.models['lgbm'] = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            valid_sets=[val_data],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        self.models['lgbm'].save_model(f"{self.model_path}lgbm_model.txt")

        # 3. CatBoost
        logger.info("Training CatBoost...")
        self.models['catboost'] = CatBoostClassifier(
            iterations=500,
            depth=6,
            learning_rate=0.01,
            loss_function='Logloss',
            random_seed=42,
            verbose=False
        )
        self.models['catboost'].fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            early_stopping_rounds=50,
            verbose=False
        )
        self.models['catboost'].save_model(f"{self.model_path}catboost_model.cbm")
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ„Ð¸Ñ‡ Ð´Ð»Ñ consistency
        self.feature_columns = list(X_train.columns)
        with open(f"{self.model_path}features.pkl", "wb") as f:
            joblib.dump(self.feature_columns, f)
            
        logger.info("âœ… All models trained and saved!")

    def predict_probability(self, features: dict) -> float:
        """
        Ð’Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ.
        Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑÐ° 1 (Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¸Ð³Ð½Ð°Ð»).
        """
        if not any(self.models.values()) or not self.feature_columns:
            logger.warning("Models not trained. Returning neutral 0.5")
            return 0.5

        try:
            # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° Ñ„Ð¸Ñ‡ Ð² Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¼ Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ
            feature_vector = []
            for col in self.feature_columns:
                feature_vector.append(features.get(col, 0.0))
            
            X = np.array(feature_vector).reshape(1, -1)
            predictions = {}
            
            # XGBoost prediction
            if self.models['xgb']:
                prob = self.models['xgb'].predict_proba(X)[0][1]
                predictions['xgb'] = prob
            
            # LightGBM prediction
            if self.models['lgbm']:
                prob = self.models['lgbm'].predict(X)[0]
                # LightGBM Ð¼Ð¾Ð¶ÐµÑ‚ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ raw score, Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ sigmoid
                prob = 1 / (1 + np.exp(-prob))
                predictions['lgbm'] = float(prob)

            # CatBoost prediction
            if self.models['catboost']:
                prob = self.models['catboost'].predict_proba(X)[0][1]
                predictions['catboost'] = prob

            # Ð’Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð¾Ðµ ÑÑ€ÐµÐ´Ð½ÐµÐµ
            if predictions:
                weighted_prob = sum(
                    pred * self.model_weights.get(name, 0.33)
                    for name, pred in predictions.items()
                )
                return float(np.clip(weighted_prob, 0.0, 1.0))
                
        except Exception as e:
            logger.error(f"ML Prediction Error: {e}")
            
        return 0.5

    def update_weights(self, model_performances: Dict[str, float]):
        """
        Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²ÐµÑÐ¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ performance.
        model_performances = {'xgb': 0.78, 'lgbm': 0.82, 'catboost': 0.75}
        """
        total = sum(model_performances.values())
        if total > 0:
            for model_name, perf in model_performances.items():
                self.model_weights[model_name] = perf / total
            logger.info(f"Updated weights: {self.model_weights}")
